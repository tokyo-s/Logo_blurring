{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the the directory where all modules are saved.\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\vladi\\OneDrive\\Desktop\\Sigmoid\\CV\\Object detection\\object detection\\object detection\\utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Importing all needed libraries.\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image,ImageDraw,ImageFilter\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "from PIL.ExifTags import TAGS, GPSTAGS\n",
    "\n",
    "import cv2\n",
    "from subprocess import call\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object detection model\n",
    "class ODM:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "            Empty constructor of the model instance\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def get_model(self, num_classes,path_to_model):\n",
    "        '''\n",
    "            This method is used to create a pretrained FastRCNN Model.\n",
    "        :num_classes: int\n",
    "            The number of classes. Should be setted n+1 wher n is the number of object to detect.\n",
    "        :path_to_model: str\n",
    "            Path to saved, pretrained model\n",
    "        '''\n",
    "        #import fasterrcnn model\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "        #get the number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "        # replace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        #load the pretrained model\n",
    "        model.load_state_dict(torch.load(path_to_model))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def detect_logos(self, path_to_model : str, path_to_video : str, path_to_save : str):\n",
    "        '''\n",
    "            This method is used to create a video where all logos from input video are blurred\n",
    "        :path_to_model: str\n",
    "            Path to saved, pretrained model\n",
    "        :path_to_video: str\n",
    "            Path to video that should be edited\n",
    "        :path_to_save: str\n",
    "            Path to folder location where to save all results\n",
    "        '''\n",
    "        \n",
    "        #searching for cuda device if has one, else use simple cpu\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        #load model using defined method\n",
    "        loaded_model = self.get_model(num_classes=2,path_to_model = path_to_model)\n",
    "        #setting device on which will model will work\n",
    "        loaded_model.to(device)\n",
    "        \n",
    "        #setting model to a state where it will just evaluate and not train\n",
    "        loaded_model.eval()\n",
    "        \n",
    "        #importing video that should be edited\n",
    "        cap = cv2.VideoCapture(path_to_video)\n",
    "        #getting fps from edited video to create another video with same fps\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        print(frame_count)\n",
    "        #create a new vidio writter with which we will save the new video\n",
    "        out = cv2.VideoWriter(os.path.join(path_to_save,'blured_25.mp4'),cv2.VideoWriter_fourcc(*'a\\0\\0\\0'), fps, (int(cap.get(3)),int(cap.get(4))))\n",
    "\n",
    "        #transform function for the frames from video to transform into an input for model\n",
    "        transform = T.Compose([T.ToTensor()])\n",
    "        \n",
    "        #count to number frames to save in a dataset the boxes\n",
    "        frame_nr = 0\n",
    "        #create dataset for classificatoin\n",
    "        boxes_df = pd.DataFrame()\n",
    "        \n",
    "        loop = tqdm(total=frame_count)\n",
    "        #while there are frames in video iterate, find logos, and blur them\n",
    "        while cap.isOpened():\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            #if success is False, break loop\n",
    "            if ret == False:\n",
    "                break\n",
    "            \n",
    "            loop.update(1)\n",
    "\n",
    "            #transform frame array to PIL image\n",
    "            img = Image.fromarray(frame).convert('RGB')\n",
    "\n",
    "            #transfomr frame from PIL image to tensor so we can send it to model\n",
    "            frame = transform(img,'0')\n",
    "            \n",
    "            #getting just the frame\n",
    "            img, _ = frame\n",
    "            \n",
    "            #send frame to object detection model and make the prediction\n",
    "            with torch.no_grad():\n",
    "                prediction = loaded_model([img.to(device)])\n",
    "\n",
    "            #Convert back to PIL image\n",
    "            image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "            \n",
    "            #iterate over the boxes\n",
    "            for element in range(len(prediction[0]['boxes'])):\n",
    "                #get the boxe's coordinates and score\n",
    "                boxes = prediction[0]['boxes'][element].cpu().numpy()\n",
    "                score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals=4)\n",
    "\n",
    "                #find logos with a precision > the a number\n",
    "                if score >= 0.25:\n",
    "\n",
    "                    #get the coordinates for the logo\n",
    "                    box = (int(boxes[0]),int(boxes[1]),int(boxes[2]),int(boxes[3]))\n",
    "                    \n",
    "                    #if score > 0.8 add logo to classification set\n",
    "                    if score >= 0.8:\n",
    "                        new_row = pd.Series([frame_nr,box[0],box[1],box[2],box[3]])\n",
    "                        boxes_df = boxes_df.append(new_row,ignore_index=True)\n",
    "\n",
    "                    #blur the logo with the box the logo has\n",
    "                    ic = image.crop(box)\n",
    "                    for i in range(50):  \n",
    "                        ic = ic.filter(ImageFilter.BLUR)\n",
    "                    image.paste(ic, box)\n",
    "\n",
    "            #convert frame to numpy array and increment frame number\n",
    "            frame = np.array(image)\n",
    "            frame_nr+=1\n",
    "\n",
    "            #write blured frame to new file\n",
    "            out.write(frame)\n",
    "            \n",
    "            #in case we want to break editing\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break \n",
    "        pbar.close()\n",
    "        #Save classification file\n",
    "        boxes_df.to_csv(os.path.join(path_to_save,'logo_boxes.csv'),index = False)\n",
    "\n",
    "        #close all files that we openned and destroy all windowses\n",
    "        out.release()\n",
    "        cap.release()   \n",
    "        cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ODM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/243.0 [00:00<?, ?it/s]c:\\python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      " 27%|█████████████████████▏                                                         | 65/243.0 [00:57<02:59,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "model.detect_logos(\n",
    "#modelul salvat\n",
    "path_to_model = r'C:\\Users\\vladi\\OneDrive\\Desktop\\Sigmoid\\CV\\Logo bluring\\logo_detection_model\\fix1_ports_model.pth', \n",
    "# path to video input\n",
    "path_to_video = r'C:\\Users\\vladi\\OneDrive\\Desktop\\Sigmoid\\CV\\Logo bluring\\video_test\\coca.mp4', \n",
    "#path where to save result video and csv result\n",
    "path_to_save = r'C:\\Users\\vladi\\OneDrive\\Desktop\\Sigmoid\\CV\\Logo bluring\\video_test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()\n",
    "cap.release()   \n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxes_df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('logo_detection_df.csv')\n",
    "# df = df[:100]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# json.loads(df.to_json(orient=\"split\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
